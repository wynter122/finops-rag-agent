"""
Docs Agent: SageMaker/Cloud Radar ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ RAG Q&A ê·¸ë˜í”„
"""

import os
from typing import Dict, Any, TypedDict, List
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_tavily import TavilySearch
from functools import lru_cache
import json

from .retriever import get_retriever


# ì „ì—­ ì²´ì¸ ìºì‹œ
_ANSWER_CHAIN = None
_QUALITY_CHAIN = None
_WEB_SEARCH_CHAIN = None

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
DOCS_ANSWER_PROMPT = """ë‹¹ì‹ ì€ AWS SageMaker ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì›ì¹™:
1. **ì •í™•ì„±**: ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
2. **êµ¬ì²´ì„±**: ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš” (ë‹¨ê³„, ì„¤ì •, ì œí•œì‚¬í•­ ë“±).
3. **êµ¬ì¡°í™”**: ë³µì¡í•œ ë‹µë³€ì€ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•˜ì„¸ìš”.
4. **í•œêµ­ì–´**: ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
5. **ì¶œì²˜ ëª…ì‹œ**: ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì„œ ì„¹ì…˜ì„ ì–¸ê¸‰í•˜ì„¸ìš”.

ì£¼ì˜ì‚¬í•­:
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
- ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ "ë¬¸ì„œì— ëª…í™•íˆ ë‚˜ì™€ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  í‘œí˜„í•˜ì„¸ìš”.
- ì½”ë“œë‚˜ ëª…ë ¹ì–´ëŠ” ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.
- URLì´ë‚˜ ì°¸ì¡° ë§í¬ê°€ ìˆë‹¤ë©´ í¬í•¨í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ê´€ë ¨ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]
{context}

ìœ„ì˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:"""

# í’ˆì§ˆ í‰ê°€ í”„ë¡¬í”„íŠ¸
QUALITY_EVALUATION_PROMPT = """ë‹¹ì‹ ì€ ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
1. **ì™„ì„±ë„**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì™„ì „í•œê°€? (0-10ì )
2. **ì •í™•ì„±**: ë‹µë³€ì´ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ê°€? (0-10ì )
3. **êµ¬ì²´ì„±**: ë‹µë³€ì´ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ê°€? (0-10ì )
4. **ëª…í™•ì„±**: ë‹µë³€ì´ ì´í•´í•˜ê¸° ì‰½ê³  ëª…í™•í•œê°€? (0-10ì )

í‰ê°€ ê²°ê³¼ëŠ” JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{{
    "overall_score": 0-10,
    "completeness": 0-10,
    "accuracy": 0-10,
    "specificity": 0-10,
    "clarity": 0-10,
    "needs_web_search": true/false,
    "reason": "í‰ê°€ ê·¼ê±°"
}}

needs_web_searchê°€ trueì¸ ê²½ìš°:
- ì „ì²´ ì ìˆ˜ê°€ 6ì  ë¯¸ë§Œì´ê±°ë‚˜
- íŠ¹ì • ì ìˆ˜ê°€ 4ì  ë¯¸ë§Œì´ê±°ë‚˜
- ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¶ˆì™„ì „í•œ ê²½ìš°

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
{answer}

[ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´]
{context_length}ì

í‰ê°€ ê²°ê³¼:"""

# ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
WEB_ANSWER_PROMPT = """ë‹¹ì‹ ì€ AWS SageMaker ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì›ì¹™:
1. **ë¬¸ì„œ ìš°ì„ **: ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
2. **ì›¹ ë³´ì™„**: ë¬¸ì„œì— ë¶€ì¡±í•œ ì •ë³´ëŠ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³´ì™„í•˜ì„¸ìš”.
3. **ì¶œì²˜ êµ¬ë¶„**: ë¬¸ì„œ ë‚´ìš©ê³¼ ì›¹ ê²€ìƒ‰ ë‚´ìš©ì„ êµ¬ë¶„í•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”.
4. **ì •í™•ì„±**: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
5. **í•œêµ­ì–´**: ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]
{docs_context}

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{web_results}

ìœ„ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:"""


def _make_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    model = "gpt-4o-mini"
    temperature = 0.1
    timeout = 30.0
    return ChatOpenAI(model=model, temperature=temperature, timeout=timeout)


def _get_answer_chain():
    """ë‹µë³€ ìƒì„± ì²´ì¸ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _ANSWER_CHAIN
    
    if _ANSWER_CHAIN is None:
        llm = _make_llm()
        prompt = ChatPromptTemplate.from_template(DOCS_ANSWER_PROMPT)
        _ANSWER_CHAIN = prompt | llm | StrOutputParser()
    
    return _ANSWER_CHAIN


def _get_quality_chain():
    """í’ˆì§ˆ í‰ê°€ ì²´ì¸ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _QUALITY_CHAIN
    
    if _QUALITY_CHAIN is None:
        llm = _make_llm()
        prompt = ChatPromptTemplate.from_template(QUALITY_EVALUATION_PROMPT)
        _QUALITY_CHAIN = prompt | llm | JsonOutputParser()
    
    return _QUALITY_CHAIN


def _get_web_search_chain():
    """ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì²´ì¸ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _WEB_SEARCH_CHAIN
    
    if _WEB_SEARCH_CHAIN is None:
        llm = _make_llm()
        prompt = ChatPromptTemplate.from_template(WEB_ANSWER_PROMPT)
        _WEB_SEARCH_CHAIN = prompt | llm | StrOutputParser()
    
    return _WEB_SEARCH_CHAIN


def _evaluate_answer_quality(question: str, answer: str, context_length: int) -> Dict[str, Any]:
    """ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
    try:
        quality_chain = _get_quality_chain()
        result = quality_chain.invoke({
            "question": question,
            "answer": answer,
            "context_length": context_length
        })
        return result
    except Exception as e:
        # í‰ê°€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "overall_score": 5,
            "completeness": 5,
            "accuracy": 5,
            "specificity": 5,
            "clarity": 5,
            "needs_web_search": True,
            "reason": f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {str(e)}"
        }


def _web_search(question: str) -> str:
    """Tavily ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
    try:
        # Tavily API í‚¤ í™•ì¸
        if not os.getenv('TAVILY_API_KEY'):
            return "ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. TAVILY_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # Tavily ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”
        search_tool = TavilySearch(max_results=5, topic="technology")
        search_results = search_tool.invoke(question)
        
        # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        if 'results' in search_results:
            for result in search_results['results']:
                formatted_results.append({
                    'title': result.get('title', ''),
                    'content': result.get('content', ''),
                    'url': result.get('url', '')
                })
        
        return json.dumps(formatted_results, ensure_ascii=False, indent=2)
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"


def _generate_enhanced_answer(question: str, docs_context: str, web_results: str) -> str:
    """ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•œ í–¥ìƒëœ ë‹µë³€ ìƒì„±"""
    try:
        web_chain = _get_web_search_chain()
        answer = web_chain.invoke({
            "question": question,
            "docs_context": docs_context,
            "web_results": web_results
        })
        return answer
    except Exception as e:
        return f"í–¥ìƒëœ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}"


class DocsAgentState(TypedDict):
    question: str
    context: str
    answer: str
    result: dict


def retrieve_node(state: DocsAgentState) -> DocsAgentState:
    """ê²€ìƒ‰ ë…¸ë“œ: ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
    retriever = get_retriever()
    context = retriever.get_relevant_context(state["question"])
    return {**state, "context": context}


def answer_node(state: DocsAgentState) -> DocsAgentState:
    """ì‘ë‹µ ë…¸ë“œ: ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„± (í’ˆì§ˆ í‰ê°€ ë° ì›¹ ê²€ìƒ‰ í¬í•¨)"""
    question = state["question"]
    context = state["context"]
    
    try:
        if not context:
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
        else:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            if isinstance(context, list):
                context_text = "\n\n".join([str(ctx) for ctx in context])
            else:
                context_text = str(context)
            
            # 1ë‹¨ê³„: ê¸°ë³¸ ë‹µë³€ ìƒì„±
            answer_chain = _get_answer_chain()
            initial_answer = answer_chain.invoke({
                "question": question,
                "context": context_text
            })
            
            # 2ë‹¨ê³„: ë‹µë³€ í’ˆì§ˆ í‰ê°€
            quality_result = _evaluate_answer_quality(
                question=question,
                answer=initial_answer,
                context_length=len(context_text)
            )
            
            # 3ë‹¨ê³„: í’ˆì§ˆì´ ë‚®ì€ ê²½ìš° ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
            if quality_result.get("needs_web_search", False):
                print(f"ğŸ” ë‹µë³€ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤ (ì ìˆ˜: {quality_result.get('overall_score', 0)}/10). ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
                
                # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
                web_results = _web_search(question)
                
                # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•œ í–¥ìƒëœ ë‹µë³€ ìƒì„±
                enhanced_answer = _generate_enhanced_answer(
                    question=question,
                    docs_context=context_text,
                    web_results=web_results
                )
                
                # í–¥ìƒëœ ë‹µë³€ì˜ í’ˆì§ˆ ì¬í‰ê°€
                enhanced_quality = _evaluate_answer_quality(
                    question=question,
                    answer=enhanced_answer,
                    context_length=len(context_text)
                )
                
                # í–¥ìƒëœ ë‹µë³€ì´ ë” ì¢‹ì€ ê²½ìš° ì‚¬ìš©
                if enhanced_quality.get("overall_score", 0) > quality_result.get("overall_score", 0):
                    answer = enhanced_answer
                    print(f"âœ… ì›¹ ê²€ìƒ‰ì„ í†µí•´ ë‹µë³€ í’ˆì§ˆì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤ (ì ìˆ˜: {enhanced_quality.get('overall_score', 0)}/10)")
                else:
                    answer = initial_answer
                    print(f"âš ï¸ ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ê¸°ì¡´ ë‹µë³€ë³´ë‹¤ ë‚˜ì§€ ì•Šì•„ ê¸°ë³¸ ë‹µë³€ì„ ìœ ì§€í•©ë‹ˆë‹¤")
            else:
                answer = initial_answer
                print(f"âœ… ë‹µë³€ í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤ (ì ìˆ˜: {quality_result.get('overall_score', 0)}/10)")
            
            # ë‹µë³€ì´ ë„ˆë¬´ ê¸¸ ê²½ìš° ìš”ì•½
            if len(answer) > 2000:
                answer = answer[:2000] + "...\n\n(ë‹µë³€ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.)"
    
    except Exception as e:
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€
        answer = f"ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”."
    
    return {**state, "answer": answer}


def format_node(state: DocsAgentState) -> DocsAgentState:
    """í¬ë§· ë…¸ë“œ: ìµœì¢… ê²°ê³¼ í¬ë§·íŒ…"""
    result = {
        "answer": state["answer"],
        "context": state["context"],
        "intent": "docs",
        "error": False
    }
    return {**state, "result": result}


# Docs Agent StateGraph ì •ì˜
graph = StateGraph(DocsAgentState)
graph.add_node("retrieve", retrieve_node)
graph.add_node("answer", answer_node)
graph.add_node("format", format_node)

graph.set_entry_point("retrieve")
graph.add_edge("retrieve", "answer")
graph.add_edge("answer", "format")
graph.add_edge("format", END)

DOCS_GRAPH = graph.compile()


def ask(question: str) -> Dict[str, Any]:
    """
    Docs Agent ì§„ì…ì : ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        
    Returns:
        ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ê²°ê³¼
    """
    try:
        state = {"question": question}
        final = DOCS_GRAPH.invoke(state)
        return final["result"]
        
    except Exception as e:
        return {
            "error": True,
            "message": f"Docs Agent ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "intent": "docs",
            "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }
