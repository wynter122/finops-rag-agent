#!/usr/bin/env python3
"""
AWS SageMaker ë¬¸ì„œ ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ
ì‹¤ì œ AWS ê³µì‹ ë¬¸ì„œë¥¼ ì›¹ì—ì„œ íŒŒì‹±í•˜ì—¬ ë¡œì»¬ì— ì €ì¥
"""

import os
import time
import requests
from pathlib import Path
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
import re
from bs4 import BeautifulSoup
import tqdm
from dotenv import load_dotenv
from doc_urls import get_combined_urls, get_urls_by_category

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class SageMakerDocScraper:
    """AWS SageMaker ë¬¸ì„œ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, base_url: str = "https://docs.aws.amazon.com/sagemaker/latest/dg/"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def get_page_content(self, url: str) -> Optional[str]:
        """ì›¹ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def clean_html_content(self, html_content: str) -> str:
        """HTML ë‚´ìš© ì •ë¦¬"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for element in soup(['script', 'style', 'nav', 'footer', 'aside']):
            element.decompose()
        
        # AWS ë¬¸ì„œ íŠ¹í™” ì •ë¦¬
        # ì‚¬ì´ë“œë°”, ë„¤ë¹„ê²Œì´ì…˜ ë“± ì œê±°
        for element in soup.find_all(['div'], class_=re.compile(r'sidebar|nav|toc|breadcrumb')):
            element.decompose()
        
        return str(soup)
    
    def extract_title_and_headers(self, html_content: str) -> Dict[str, Any]:
        """ì œëª©ê³¼ í—¤ë” ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = ""
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
        
        # H1, H2, H3 í—¤ë” ì¶”ì¶œ
        headers = {
            'H1': [],
            'H2': [],
            'H3': []
        }
        
        for h1 in soup.find_all('h1'):
            headers['H1'].append(h1.get_text().strip())
        
        for h2 in soup.find_all('h2'):
            headers['H2'].append(h2.get_text().strip())
        
        for h3 in soup.find_all('h3'):
            headers['H3'].append(h3.get_text().strip())
        
        return {
            'title': title,
            'headers': headers
        }
    
    def scrape_sagemaker_docs(self, doc_urls: List[str]) -> List[Dict[str, Any]]:
        """SageMaker ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ - ê²°ê³¼ë¥¼ ë¦¬í„´ë§Œ í•˜ê³  ì €ì¥í•˜ì§€ ì•ŠìŒ"""
        print(f"ì›¹ ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
        print(f"ëŒ€ìƒ URL ìˆ˜: {len(doc_urls)}")
        
        scraped_docs = []
        
        for url in tqdm.tqdm(doc_urls, desc="ë¬¸ì„œ ìŠ¤í¬ë˜í•‘"):
            try:
                # í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                html_content = self.get_page_content(url)
                if not html_content:
                    continue
                
                # HTML ì •ë¦¬
                cleaned_html = self.clean_html_content(html_content)
                
                # ì œëª©ê³¼ í—¤ë” ì¶”ì¶œ
                title_headers = self.extract_title_and_headers(html_content)
                
                # íŒŒì¼ëª… ìƒì„±
                parsed_url = urlparse(url)
                filename = Path(parsed_url.path).stem
                if not filename:
                    filename = "index"
                
                # ë¬¸ì„œ ì •ë³´ë§Œ ì €ì¥ (íŒŒì¼ ì €ì¥í•˜ì§€ ì•ŠìŒ)
                doc_info = {
                    'url': url,
                    'filename': f"{filename}.html",
                    'html_content': cleaned_html,  # HTML ë‚´ìš©ë„ í¬í•¨
                    'title': title_headers['title'],
                    'headers': title_headers['headers'],
                    'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')
                }
                scraped_docs.append(doc_info)
                
                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (AWS ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(1)
                
            except Exception as e:
                print(f"ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ({url}): {e}")
                continue
        
        print(f"ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(scraped_docs)}ê°œ ë¬¸ì„œ")
        
        return scraped_docs


# get_sagemaker_doc_urls í•¨ìˆ˜ëŠ” doc_urls.pyì—ì„œ importí•˜ì—¬ ì‚¬ìš©


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ AWS SageMaker ë¬¸ì„œ ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = SageMakerDocScraper()
    
    # ë¬¸ì„œ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ë¶„ì„ ë°ì´í„°ì—ì„œë§Œ ì¶”ì¶œ)
    doc_urls = get_combined_urls()
    
    print(f"ğŸ“‹ ìŠ¤í¬ë˜í•‘í•  SageMaker ë¬¸ì„œ URL: {len(doc_urls)}ê°œ")
    print("ğŸ”¹ URL ì†ŒìŠ¤:")
    print("  - ë¶„ì„ ë°ì´í„°ì—ì„œ ë™ì ìœ¼ë¡œ ì¶”ì¶œëœ URL")
    print("  - ì¤‘ë³µ ì œê±° ë° ì •ê·œí™” ì™„ë£Œ")
    
    # URL ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 10ê°œë§Œ)
    for i, url in enumerate(doc_urls[:10], 1):
        print(f"  {i:2d}. {url}")
    if len(doc_urls) > 10:
        print(f"  ... ë° {len(doc_urls) - 10}ê°œ ë”")
    
    # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    scraped_docs = scraper.scrape_sagemaker_docs(doc_urls)
    
    print(f"\nâœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
    print(f"ğŸ“„ ë¬¸ì„œ ìˆ˜: {len(scraped_docs)}")
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“‹ ìŠ¤í¬ë˜í•‘ëœ ë¬¸ì„œë“¤:")
    for doc in scraped_docs:
        print(f"  - {doc['title']} ({doc['filename']})")
    
    return scraped_docs


if __name__ == "__main__":
    main()
