#!/usr/bin/env python3
"""
ì›¹ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ì§€ì •ëœ URLì˜ ì›¹ ë¬¸ì„œ êµ¬ì¡°ë¥¼ ë¶„ì„
"""

import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
from pathlib import Path
from typing import List, Dict, Any
import json
import argparse
import sys
import os

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ë£¨íŠ¸ì˜ data/web_structure/)
DATA_DIR = Path(__file__).parent.parent.parent.parent / "data" / "web_structure"

def analyze_web_structure(url: str = None):
    """ì›¹ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„"""
    
    # ê¸°ë³¸ URL ì„¤ì •
    if url is None:
        url = "https://docs.aws.amazon.com/sagemaker/latest/dg/machine-learning-environments.html"
    
    print(f"ğŸ” ì›¹ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì‹œì‘")
    print(f"ğŸ“„ ë¶„ì„ ëŒ€ìƒ: {url}")
    print("=" * 80)
    
    # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    
    try:
        response = session.get(url, timeout=10)
        response.raise_for_status()
        html_content = response.text
    except Exception as e:
        print(f"âŒ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return
    
    # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 1. ì œëª©ê³¼ í—¤ë” êµ¬ì¡° ë¶„ì„
    print("ğŸ“‹ 1. ë¬¸ì„œ ì œëª© ë° í—¤ë” êµ¬ì¡°")
    print("-" * 50)
    
    title = soup.find('title')
    if title:
        print(f"ì œëª©: {title.get_text().strip()}")
    
    # H1, H2, H3 í—¤ë” ì¶”ì¶œ
    headers = {
        'H1': [],
        'H2': [],
        'H3': []
    }
    
    for h1 in soup.find_all('h1'):
        headers['H1'].append(h1.get_text().strip())
    
    for h2 in soup.find_all('h2'):
        headers['H2'].append(h2.get_text().strip())
    
    for h3 in soup.find_all('h3'):
        headers['H3'].append(h3.get_text().strip())
    
    print(f"H1 í—¤ë” ({len(headers['H1'])}ê°œ):")
    for h1 in headers['H1']:
        print(f"  - {h1}")
    
    print(f"\nH2 í—¤ë” ({len(headers['H2'])}ê°œ):")
    for h2 in headers['H2']:
        print(f"  - {h2}")
    
    print(f"\nH3 í—¤ë” ({len(headers['H3'])}ê°œ):")
    for h3 in headers['H3']:
        print(f"  - {h3}")
    
    # 2. ë§í¬ êµ¬ì¡° ë¶„ì„
    print(f"\nğŸ”— 2. ë¬¸ì„œ ë‚´ ë§í¬ êµ¬ì¡° ë¶„ì„")
    print("-" * 50)
    
    base_url = "https://docs.aws.amazon.com/sagemaker/latest/dg/"
    sagemaker_links = []
    
    # ëª¨ë“  ë§í¬ ì°¾ê¸°
    for link in soup.find_all('a', href=True):
        href = link.get('href')
        link_text = link.get_text().strip()
        
        # SageMaker ê´€ë ¨ ë§í¬ë§Œ í•„í„°ë§
        if href and ('sagemaker' in href.lower() or 'sagemaker' in link_text.lower()):
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            if href.startswith('/'):
                full_url = urljoin(base_url, href)
            elif href.startswith('http'):
                full_url = href
            else:
                full_url = urljoin(base_url, href)
            
            sagemaker_links.append({
                'text': link_text,
                'url': full_url,
                'href': href
            })
    
    print(f"ë°œê²¬ëœ SageMaker ê´€ë ¨ ë§í¬ ({len(sagemaker_links)}ê°œ):")
    for link in sagemaker_links:
        print(f"  - {link['text']} -> {link['url']}")
    
    # 3. ì£¼ìš” ì„¹ì…˜ ë° ì„œë¹„ìŠ¤ ë¶„ì„
    print(f"\nğŸ“š 3. ì£¼ìš” SageMaker ì„œë¹„ìŠ¤ ë° í™˜ê²½ ë¶„ì„")
    print("-" * 50)
    
    # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì£¼ìš” ì„œë¹„ìŠ¤ ì¶”ì¶œ
    content_text = soup.get_text()
    
    # ì£¼ìš” SageMaker ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ
    services = [
        'Amazon SageMaker Studio',
        'Amazon SageMaker Studio Classic', 
        'Amazon SageMaker Notebook Instances',
        'Amazon SageMaker Studio Lab',
        'Amazon SageMaker Canvas',
        'Amazon SageMaker geospatial',
        'RStudio on Amazon SageMaker',
        'SageMaker HyperPod',
        'Amazon SageMaker AutoPilot',
        'Amazon SageMaker Feature Store',
        'Amazon SageMaker Model Monitor',
        'Amazon SageMaker Pipelines',
        'Amazon SageMaker JumpStart',
        'Amazon SageMaker Ground Truth',
        'Amazon SageMaker Data Wrangler',
        'Amazon SageMaker Model Registry',
        'Amazon SageMaker Projects',
        'Amazon SageMaker ML Lineage',
        'Amazon SageMaker Model Building Pipeline',
        'Amazon SageMaker Training Compiler',
        'Amazon SageMaker Inference Recommender',
        'Amazon SageMaker Edge Manager',
        'Amazon SageMaker Neo',
        'Amazon SageMaker Clarify',
        'Amazon SageMaker Debugger'
    ]
    
    found_services = []
    for service in services:
        if service.lower() in content_text.lower():
            found_services.append(service)
    
    print(f"ë¬¸ì„œì—ì„œ ë°œê²¬ëœ SageMaker ì„œë¹„ìŠ¤ ({len(found_services)}ê°œ):")
    for service in found_services:
        print(f"  - {service}")
    
    # 4. Topics ì„¹ì…˜ ë¶„ì„
    print(f"\nğŸ“– 4. Topics ì„¹ì…˜ ë¶„ì„")
    print("-" * 50)
    
    # Topics ì„¹ì…˜ ì°¾ê¸°
    topics_section = soup.find('div', class_='topics')
    if topics_section:
        topic_links = topics_section.find_all('a')
        print(f"Topics ì„¹ì…˜ì—ì„œ ë°œê²¬ëœ ë§í¬ ({len(topic_links)}ê°œ):")
        for link in topic_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            if href:
                full_url = urljoin(base_url, href)
                print(f"  - {text} -> {full_url}")
    else:
        print("Topics ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # 5. ë¬¸ì„œ êµ¬ì¡° ìš”ì•½
    print(f"\nğŸ“Š 5. ë¬¸ì„œ êµ¬ì¡° ìš”ì•½")
    print("-" * 50)
    
    summary = {
        'title': title.get_text().strip() if title else 'Unknown',
        'headers': headers,
        'sagemaker_links_count': len(sagemaker_links),
        'found_services_count': len(found_services),
        'found_services': found_services,
        'sagemaker_links': sagemaker_links
    }
    
    print(f"ë¬¸ì„œ ì œëª©: {summary['title']}")
    print(f"ì´ í—¤ë” ìˆ˜: H1({len(headers['H1'])}), H2({len(headers['H2'])}), H3({len(headers['H3'])}")
    print(f"SageMaker ê´€ë ¨ ë§í¬: {len(sagemaker_links)}ê°œ")
    print(f"ë°œê²¬ëœ ì„œë¹„ìŠ¤: {len(found_services)}ê°œ")
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (docs_agent/data/ ë””ë ‰í† ë¦¬ì—)
    DATA_DIR.mkdir(exist_ok=True)
    
    parsed_url = urlparse(url)
    filename = Path(parsed_url.path).stem
    if not filename:
        filename = "analysis"
    
    output_filename = DATA_DIR / f"web_structure_analysis_{filename}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return summary


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬"""
    parser = argparse.ArgumentParser(
        description="ì›¹ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python analyze_web_structure.py
  python analyze_web_structure.py --url "https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html"
  python analyze_web_structure.py -u "https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html"
        """
    )
    
    parser.add_argument(
        '--url', '-u',
        type=str,
        help='ë¶„ì„í•  ì›¹ ë¬¸ì„œ URL (ê¸°ë³¸ê°’: machine-learning-environments.html)'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        help='ê²°ê³¼ JSON íŒŒì¼ëª… (ê¸°ë³¸ê°’: web_structure_analysis_{filename}.json)'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='ìƒì„¸í•œ ë¶„ì„ ì •ë³´ ì¶œë ¥'
    )
    
    args = parser.parse_args()
    
    try:
        # URL ë¶„ì„ ì‹¤í–‰
        result = analyze_web_structure(args.url)
        
        if args.verbose:
            print(f"\nğŸ” ìƒì„¸ ë¶„ì„ ê²°ê³¼:")
            print(f"ë¶„ì„ URL: {args.url or 'ê¸°ë³¸ URL'}")
            print(f"ë¬¸ì„œ ì œëª©: {result['title']}")
            print(f"ì´ ë§í¬ ìˆ˜: {result['sagemaker_links_count']}")
            print(f"ë°œê²¬ëœ ì„œë¹„ìŠ¤ ìˆ˜: {result['found_services_count']}")
        
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
